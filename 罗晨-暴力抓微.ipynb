{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、抓取前的准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 下载好[Fiddler for .NET4](https://www.telerik.com/download/fiddler)，并进行安装\n",
    "\n",
    "    - Fiddler的工作原理如下\n",
    "    \n",
    ">* 电脑与互联网之间的通信通过不同的数据包收发来实现，Fiddler可以从中间对数据进行__拦截__，拷贝一份数据后再将数据发送给目的端（Server Side）。<br/>\n",
    ">同类的还有__WireShark__，可以抓取更广泛的数据包，Fiddler主要抓取网页的数据包。<br/>\n",
    ">* Fiddler安装：__一直点击“下一步”__。<br/>\n",
    ">安装好Fiddler后的配置：点击WinConfig——勾选__IE（windows_ie_ac_001）__——在IE中浏览网页——在Fiddler中从左列拖拽数据到右上方的__Composer__看具体的传输信息。<br/>\n",
    ">* 打开IE，在IE中查看新浪微博网页版，weibo.cn。<br/>\n",
    ">如果在登录weibo.cn时出现“检验码不正确”的问题时，可以现在IE中登录weibo.com，登录时选择“记住我”，然后再开启新窗口输入weibo.cn即可正常打开。<br/>\n",
    ">成功访问weibo.cn后，在Fiddler中会出现weibo.cn（<> 898 200 HTTP weibo.cn），把这个拖至右上角，标签项打开__Composer__，其中包含一个Cookie，请复制粘贴下来，待会要用到。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、核心的抓取代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.首先声明编码格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding=utf-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.在terminal中安装好两个基本库，方法为：<br/>\n",
    "    - pip install lxml\n",
    "    - pip install requests\n",
    "安装完成后，需要引入这两个库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests, time\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.将你在本教程开始时获取的cookie粘贴到如下的 XXXXXXXX 位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cookies = {'Cookie' : 'XXXXXXXX'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.将你要抓取的微博地址粘贴到如下的 XXXXXXXX 位置，此处以\"人民日报\"账号为例，\"人民日报\"的第一页链接为：<br/>\n",
    "http://weibo.cn/rmrb?page=1<br/>\n",
    "此时，你粘贴的部分为应当为：<br/>\n",
    "http://weibo.cn/rmrb?page=<br/>\n",
    "也即，后面的数字\"1\"请删去，如果在筛选中指定了时间段同理，粘贴地址保存到\"page=\"才可以。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'XXXXXXXX'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.在如下的 XXXXXXXX 处指定你的抓取页数（页数可以在weibo.cn的最下方查看到）<br/>\n",
    "以及在如下的 ** 处指定你抓取数据的保存路径（如：/Users/Luo Chen/Desktop/name.txt）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page = int(XXXXXXXX)\n",
    "f = open('********', 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.以下代码采用节点定位的方式来抓取微博内容，涉及知识较多，因此不进行详细解释，只需执行即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1, page + 1):\n",
    "    new_url = url + str(i)\n",
    "    source = requests.get(new_url, cookies = cookies)\n",
    "    HTML = bytes(bytearray(source.text, encoding = 'utf-8'))\n",
    "    html = etree.HTML(HTML)\n",
    "    p_div = html.xpath('//div[@class = \"c\"]')\n",
    "    parent_div = p_div[:-2]\n",
    "    for div in parent_div:\n",
    "        div_1 = div[0].xpath('string(.)')\n",
    "        if len(div) == 2:\n",
    "            div_2 = div[1].xpath('string(.)')\n",
    "        else:\n",
    "            div_2 = ''\n",
    "        if len(div) == 3:\n",
    "            div_3 = div[2].xpath('string(.)')\n",
    "        else:\n",
    "            div_3 = ''\n",
    "        if len(div) == 4:\n",
    "            div_4 = div[3].xpath('string(.)')\n",
    "        else:\n",
    "            div_4 = ''\n",
    "        print div_1 + '$' + div_2 + '$' + div_3 + '$' + div_4\n",
    "        time.sleep(3)\n",
    "        f.write(div_1.encode('utf-8') + '$' + div_2.encode('utf-8') + '$' + div_3.encode('utf-8') + '$' + div_4.encode('utf-8') + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、执行结束后的处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 代码正常运行结束后，在你设定的保存路径的文件里即可浏览抓取下来的微博数据\n",
    "- 数据用 \"$\" 进行分隔，可以将数据导入Excel中，利用 \"数据\" —— \"分列\" 来将抓取数据整理为结构化数据。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
